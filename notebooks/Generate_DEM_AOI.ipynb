{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate DEM from USGS 3D Elevation Program (3DEP) lidar data for user-defined area of interest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"Author\"></a>\n",
    "## Author\n",
    "Bishal Dhungana"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"Install-and-run-on-local-file-system\"></a>\n",
    "### Install and run on local file system\n",
    "\n",
    "Because of the file size, it is better to run the Jupyter Notebook on your local machine:\n",
    "\n",
    "Make a new directory on your local file system where the post_lidar Jupyter Notebooks (and all products data, if desired) will be saved. In this example case, the directory will be called `post_lidar`.\n",
    "\n",
    "    $ mkdir post_lidar\n",
    "\n",
    "Change into the new directory and `git clone` the dev branch in the Github repository containing the Jupyter Notebooks and other relevant files to your local file system.\n",
    "\n",
    "    $ cd post_lidar\n",
    "    \n",
    "    $ git clone \n",
    "\n",
    "Next, create a virtual environment with the required dependencies, using the `environment.yml` file contained in the cloned Github repo. Note: Executing the following command will automatically create the conda environment with name `venv` and all of the required dependencies installed. If you would prefer a different name, replace `venv` with another name in the following command:\n",
    "\n",
    "    $ conda env create -n venv --file environment.yml\n",
    "\n",
    "Activate the conda environment with all of the necessary dependencies installed:\n",
    "\n",
    "    $ conda activate venv\n",
    "\n",
    "Now, launch the chosen Jupyter Notebook. If unsure how to launch a Notebook, refer to this guide (https://jupyter-notebook-beginner-guide.readthedocs.io/en/latest/execute.html).\n",
    "\n",
    "**You may now proceed to Library Imports**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"Library-Imports\"></a>\n",
    "### Library Imports\n",
    "\n",
    "After successfully completing the steps outlined above, we can now import the modules for use throughout the rest of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Modules\n",
    "import copy\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import re\n",
    "import subprocess\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import ipyleaflet\n",
    "import ipywidgets as widgets\n",
    "import pdal\n",
    "import pyproj\n",
    "import requests\n",
    "import rasterio\n",
    "import richdem as rd\n",
    "from osgeo import gdal, ogr, osr\n",
    "from shapely.geometry import shape, Point, Polygon\n",
    "from shapely.ops import transform\n",
    "\n",
    "import rioxarray as rxr\n",
    "from rasterio.enums import Resampling\n",
    "import matplotlib.pyplot as plt\n",
    "import rasterio.warp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define AOI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter shapefile path, if applicable. Example: shapefile_path = '/path/to/shapefile.shp'.\n",
    "# Otherwise leave as shapefile_path = ''\n",
    "# Run this cell either way, or next cell will not run appropriately.\n",
    "shapefile_path = \"../shp/Himmel.shp\" # /path/to/your/shapefile/shapefile.shp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"Define-Functions\"></a>\n",
    "### Define Functions\n",
    "\n",
    "Several functions are provided in the cell below. These functions are necessary for successful execution of remainder of the notebook. Broadly, these functions provide the utility for the user to draw and area of interest (AOI) on an interactive map and construct the PDAL pipeline for getting the point cloud data from the Amazon Web Services EPT bucket, performing processing steps, producing DEM, and saving the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proj_to_3857(poly, orig_crs):\n",
    "    wgs84 = pyproj.CRS(\"EPSG:4326\")\n",
    "    web_mercator = pyproj.CRS(\"EPSG:3857\")\n",
    "    project_gcs = pyproj.Transformer.from_crs(orig_crs, wgs84, always_xy=True).transform\n",
    "    project_wm = pyproj.Transformer.from_crs(orig_crs, web_mercator, always_xy=True).transform\n",
    "    user_poly_proj4326 = transform(project_gcs, poly)\n",
    "    user_poly_proj3857 = transform(project_wm, poly)\n",
    "    return(user_poly_proj4326, user_poly_proj3857)\n",
    "\n",
    "def gcs_to_proj(poly):\n",
    "    wgs84 = pyproj.CRS(\"EPSG:4326\")\n",
    "    web_mercator = pyproj.CRS(\"EPSG:3857\")\n",
    "    project = pyproj.Transformer.from_crs(wgs84, web_mercator, always_xy=True).transform\n",
    "    user_poly_proj3857 = transform(project, poly)\n",
    "    return(user_poly_proj3857)\n",
    "\n",
    "def import_shapefile_to_shapely(path):\n",
    "    shapefile_path = path\n",
    "    gdf = gpd.read_file(shapefile_path)\n",
    "    orig_crs = gdf.crs                   # this is the original CRS of the imported shapefile\n",
    "    user_shp = gdf.loc[0, 'geometry']\n",
    "    user_shp_epsg4326, user_shp_epsg3857 = proj_to_3857(user_shp, orig_crs)\n",
    "    user_AOI = [[user_shp_epsg4326, user_shp_epsg3857]]\n",
    "    return user_AOI\n",
    "    \n",
    "def handle_draw(target, action, geo_json):      \n",
    "    geom = dict(geo_json['geometry'])\n",
    "    user_poly = shape(geom)\n",
    "    user_poly_proj3857 = gcs_to_proj(user_poly)\n",
    "    print('AOI is valid and has boundaries of ', user_poly_proj3857.bounds, 'Please proceed to the next cell.')\n",
    "    user_AOI.append((user_poly, user_poly_proj3857))  #for various reasons, we need user AOI in GCS and EPSG 3857\n",
    "    \n",
    "def downsample_dem(dem, target_resolution=10):\n",
    "    # Get the current resolution\n",
    "    current_resolution_x = abs(dem.rio.resolution()[0])\n",
    "    current_resolution_y = abs(dem.rio.resolution()[1])\n",
    "    \n",
    "    # Calculate the scale factors\n",
    "    scale_factor_x = current_resolution_x / target_resolution\n",
    "    scale_factor_y = current_resolution_y / target_resolution\n",
    "    \n",
    "    # Calculate the new width and height based on the scale factors\n",
    "    new_width = int(dem.rio.width * scale_factor_x)\n",
    "    new_height = int(dem.rio.height * scale_factor_y)\n",
    "    \n",
    "    # Downsample DTM/DSM\n",
    "    down_sampled = dem.rio.reproject(\n",
    "        dem.rio.crs,\n",
    "        shape=(new_height, new_width),\n",
    "        resampling=Resampling.bilinear\n",
    "    )\n",
    "    \n",
    "    return down_sampled\n",
    "    \n",
    "def build_pdal_pipeline(extent_epsg3857, usgs_3dep_dataset_names, pc_resolution, filterNoise = False,\n",
    "                        reclassify = False, savePointCloud = True, outCRS = 3857, pc_outName = 'filter_test',\n",
    "                        pc_outType = 'laz'):\n",
    "\n",
    "    #this is the basic pipeline which only accesses the 3DEP data\n",
    "    readers = []\n",
    "    for name in usgs_3dep_dataset_names:\n",
    "        url = \"https://s3-us-west-2.amazonaws.com/usgs-lidar-public/{}/ept.json\".format(name)\n",
    "        reader = {\n",
    "            \"type\": \"readers.ept\",\n",
    "            \"filename\": str(url),\n",
    "            \"polygon\": str(extent_epsg3857),\n",
    "            \"requests\": 3,\n",
    "            \"resolution\": pc_resolution\n",
    "        }\n",
    "        readers.append(reader)\n",
    "        \n",
    "    pointcloud_pipeline = {\n",
    "            \"pipeline\":\n",
    "                readers\n",
    "    }\n",
    "    \n",
    "    if filterNoise == True:\n",
    "        \n",
    "        # Filter stage for class 7\n",
    "        filter_stage_class7 = {\n",
    "            \"type\": \"filters.range\",\n",
    "            \"limits\": \"Classification![7:7]\"\n",
    "        }\n",
    "\n",
    "        # Filter stage for class 18\n",
    "        filter_stage_class18 = {\n",
    "            \"type\": \"filters.range\",\n",
    "            \"limits\": \"Classification![18:18]\"\n",
    "        }\n",
    "\n",
    "        # Append both filter stages to the pipeline separately\n",
    "        pointcloud_pipeline['pipeline'].append(filter_stage_class7)\n",
    "        pointcloud_pipeline['pipeline'].append(filter_stage_class18)\n",
    "    \n",
    "    if reclassify == True:\n",
    "        \n",
    "        remove_classes_stage = {\n",
    "            \"type\":\"filters.assign\",\n",
    "            \"value\":\"Classification = 0\"\n",
    "        }\n",
    "        \n",
    "        classify_ground_stage = {\n",
    "            \"type\":\"filters.smrf\"\n",
    "        }\n",
    "        \n",
    "        reclass_stage = {\n",
    "            \"type\":\"filters.range\",\n",
    "            \"limits\":\"Classification[2:2]\"\n",
    "        }\n",
    "\n",
    "        pointcloud_pipeline['pipeline'].append(remove_classes_stage)\n",
    "        pointcloud_pipeline['pipeline'].append(classify_ground_stage)\n",
    "        pointcloud_pipeline['pipeline'].append(reclass_stage)\n",
    "        \n",
    "    reprojection_stage = {\n",
    "        \"type\":\"filters.reprojection\",\n",
    "        \"out_srs\":\"EPSG:{}\".format(outCRS)\n",
    "    }\n",
    "    \n",
    "    pointcloud_pipeline['pipeline'].append(reprojection_stage)\n",
    "    \n",
    "    if savePointCloud == True:\n",
    "        \n",
    "        if pc_outType == 'las':\n",
    "            savePC_stage = {\n",
    "                \"type\": \"writers.las\",\n",
    "                \"filename\": str(pc_outName)+'.'+ str(pc_outType),\n",
    "            }\n",
    "        elif pc_outType == 'laz':    \n",
    "            savePC_stage = {\n",
    "                \"type\": \"writers.las\",\n",
    "                \"compression\": \"laszip\",\n",
    "                \"filename\": str(pc_outName)+'.'+ str(pc_outType),\n",
    "            }\n",
    "        else:\n",
    "            raise Exception(\"pc_outType must be 'las' or 'laz'.\")\n",
    "\n",
    "        pointcloud_pipeline['pipeline'].append(savePC_stage)\n",
    "        \n",
    "    return pointcloud_pipeline\n",
    "\n",
    "def make_DEM_pipeline(extent_epsg3857, usgs_3dep_dataset_name, pc_resolution, dem_resolution,\n",
    "                      filterNoise=True, reclassify=False, savePointCloud=False, outCRS=3857,\n",
    "                      pc_outName='filter_test', pc_outType='laz', demType='dtm', gridMethod='idw', \n",
    "                      dem_outName='dem_test', dem_outExt='tif', driver=\"GTiff\"):\n",
    "    \n",
    "    dem_pipeline = build_pdal_pipeline(extent_epsg3857, usgs_3dep_dataset_name, pc_resolution,\n",
    "                                       filterNoise, reclassify, savePointCloud, outCRS, pc_outName, pc_outType)\n",
    "    \n",
    "    if demType == 'dsm':\n",
    "        dem_stage = {\n",
    "            \"type\": \"writers.gdal\",\n",
    "            \"filename\": str(dem_outName) + '.' + str(dem_outExt),\n",
    "            \"gdaldriver\": driver,\n",
    "            \"nodata\": -9999,\n",
    "            \"output_type\": gridMethod,\n",
    "            \"resolution\": float(dem_resolution),\n",
    "            \"gdalopts\": \"COMPRESS=LZW,TILED=YES,blockxsize=256,blockysize=256,COPY_SRC_OVERVIEWS=YES\"\n",
    "        }\n",
    "    \n",
    "    elif demType == 'dtm':\n",
    "        groundfilter_stage = {\n",
    "            \"type\": \"filters.range\",\n",
    "            \"limits\": \"Classification[2:2]\"\n",
    "        }\n",
    "\n",
    "        dem_pipeline['pipeline'].append(groundfilter_stage)\n",
    "\n",
    "        dem_stage = {\n",
    "            \"type\": \"writers.gdal\",\n",
    "            \"filename\": str(dem_outName) + '.' + str(dem_outExt),\n",
    "            \"gdaldriver\": driver,\n",
    "            \"nodata\": -9999,\n",
    "            \"output_type\": gridMethod,\n",
    "            \"resolution\": float(dem_resolution),\n",
    "            \"gdalopts\": \"COMPRESS=LZW,TILED=YES,blockxsize=256,blockysize=256,COPY_SRC_OVERVIEWS=YES\"\n",
    "        }\n",
    "    \n",
    "    else:\n",
    "        raise Exception(\"demType must be 'dsm' or 'dtm'.\")\n",
    "        \n",
    "    dem_pipeline['pipeline'].append(dem_stage)\n",
    "    \n",
    "    return dem_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"Data-Access-and-Processing\"></a>\n",
    "## Data Access and Processing\n",
    "Now that we have the required modules imported and functions defined, we can proceed with defining our area of interest (AOI), accessing/processing the 3DEP data from the Amazon Web Services EPT bucket. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get GeoJSON file for 3DEP outlines from URL \n",
    "\n",
    "print(\"Requesting, loading, and projecting 3DEP dataset polygons...\")\n",
    "\n",
    "#request the boundaries from the Github repo and save locally.\n",
    "url = 'https://raw.githubusercontent.com/hobuinc/usgs-lidar/master/boundaries/resources.geojson'\n",
    "r = requests.get(url, verify=False)\n",
    "with open('resources.geojson', 'w') as f:\n",
    "    f.write(r.content.decode(\"utf-8\"))\n",
    "\n",
    "with open('resources.geojson', 'r') as f:\n",
    "    geojsons_3DEP = json.load(f)\n",
    "    \n",
    "#make pandas dataframe and create pandas.Series objects for the names, urls, and number of points for each boundary.\n",
    "with open('resources.geojson', 'r') as f:\n",
    "    df = gpd.read_file(f)\n",
    "    names = df['name']\n",
    "    urls = df['url']\n",
    "    num_points = df['count']\n",
    "\n",
    "#project the boundaries to EPSG 3857 (necessary for API call to AWS for 3DEP data)\n",
    "projected_geoms = []\n",
    "for geometry in df['geometry']:\n",
    "        projected_geoms.append(gcs_to_proj(geometry))\n",
    "\n",
    "geometries_GCS = df['geometry']\n",
    "geometries_EPSG3857 = gpd.GeoSeries(projected_geoms)\n",
    "\n",
    "print('Done. 3DEP polygons downloaded and projected to Web Mercator (EPSG:3857)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you don't have a shapefile, select an area of interest using the tools on the left side of the map.\n",
    "\n",
    "m = ipyleaflet.Map(\n",
    "    basemap=ipyleaflet.basemaps.Esri.WorldTopoMap,\n",
    "    center=(37, -100),\n",
    "    zoom=3.5,\n",
    "    crs=ipyleaflet.projections.EPSG3857\n",
    "    )\n",
    "\n",
    "geo_json_3DEP = ipyleaflet.GeoJSON(data=geojsons_3DEP, style = {'color': 'green', 'opacity':1, \n",
    "                                       'weight':1, 'fillOpacity':0.1})\n",
    "\n",
    "m.add_layer(geo_json_3DEP)  #add 3DEP polygons GeoJSON\n",
    "\n",
    "dc = ipyleaflet.DrawControl(\n",
    "    polygon={\"shapeOptions\": {\"color\": \"blue\"}},\n",
    "    rectangle={\"shapeOptions\": {\"color\": \"blue\"}},\n",
    "    circlemarker={},\n",
    "    polyline={}\n",
    ")\n",
    "\n",
    "if os.path.exists(shapefile_path):\n",
    "    user_AOI = import_shapefile_to_shapely(shapefile_path)\n",
    "    print('shapefile loaded. proceed to next cell')\n",
    "    \n",
    "else:\n",
    "    print('Select an Area of Interest using the tools on the left side of the map.')\n",
    "    user_AOI = []\n",
    "    dc.on_draw(handle_draw)\n",
    "    m.add_control(dc)\n",
    "    display(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to find the most recent 3DEP data that covers the AOI. The aim is to get the most recent as well as the data from a single vendor or flight.\n",
    "# If you want to get all the data regardless of vendor or year, comment out the next cell.\n",
    "\n",
    "# Function to extract year from URL\n",
    "def extract_year_from_url(url):\n",
    "    match = re.search(r'(\\d{4})', url)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    return None\n",
    "\n",
    "AOI_GCS = user_AOI[-1][0]\n",
    "AOI_StatePlane = user_AOI[-1][1]\n",
    "\n",
    "intersecting_polys = []\n",
    "\n",
    "# Collect intersecting polygons with their years\n",
    "polygons_with_years = []\n",
    "for i, geom in enumerate(geometries_StatePlane):\n",
    "    if geom.intersects(AOI_StatePlane):\n",
    "        current_year = extract_year_from_url(urls[i])\n",
    "        if current_year:\n",
    "            polygons_with_years.append((current_year, names[i], geometries_GCS[i], geometries_StatePlane[i], urls[i], num_points[i]))\n",
    "\n",
    "# Sort polygons by year in descending order\n",
    "polygons_with_years.sort(reverse=True, key=lambda x: x[0])\n",
    "\n",
    "# Check if the AOI is completely covered by the latest polygons\n",
    "covered_area = Polygon()\n",
    "for poly in polygons_with_years:\n",
    "    if covered_area.covers(AOI_StatePlane):\n",
    "        break\n",
    "    covered_area = covered_area.union(poly[3])\n",
    "    intersecting_polys.append(poly)\n",
    "\n",
    "# Print the intersecting polygons\n",
    "for poly in intersecting_polys:\n",
    "    poly_json = json.dumps({\n",
    "        \"name\": poly[1],\n",
    "        \"geometry_GCS\": poly[2].__geo_interface__,\n",
    "        \"geometry_StatePlane\": poly[3].__geo_interface__,\n",
    "        \"url\": poly[4],\n",
    "        \"num_points\": int(poly[5])\n",
    "    }, indent=4)\n",
    "    print(poly_json)\n",
    "\n",
    "# Proceed to the next step if AOI falls completely within the polygon\n",
    "if covered_area.covers(AOI_StatePlane):\n",
    "    print(\"AOI is completely covered by the polygons. Proceeding to the next step.\")\n",
    "else:\n",
    "    print(\"AOI is not completely covered by the polygons. Additional polygons may be needed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # If you want to get all the data regardless of vendor or year, comment out this cell. \n",
    "\n",
    "# AOI_GCS = user_AOI[-1][0]\n",
    "# AOI_EPSG3857 = user_AOI[-1][1]\n",
    "\n",
    "# intersecting_polys = []\n",
    "\n",
    "# for i,geom in enumerate(geometries_EPSG3857):\n",
    "#     if geom.intersects(AOI_EPSG3857):\n",
    "#         intersecting_polys.append((names[i], geometries_GCS[i], geometries_EPSG3857[i], urls[i], num_points[i]))\n",
    "        \n",
    "# print(intersecting_polys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find AOI center for plotting purposes\n",
    "centroid = list(AOI_GCS.centroid.coords)[0]\n",
    "\n",
    "# Make ipyleaflet map\n",
    "m = ipyleaflet.Map(\n",
    "    basemap=ipyleaflet.basemaps.Esri.WorldTopoMap,\n",
    "    center=(centroid[1], centroid[0]),\n",
    "    zoom=12,\n",
    ")\n",
    "\n",
    "# Add intersecting 3DEP polygon(s) to the map\n",
    "wlayer_3DEP_list = []\n",
    "usgs_3dep_datasets = []\n",
    "number_pts_est = []\n",
    "for i, poly in enumerate(intersecting_polys):\n",
    "    wlayer_3DEP = ipyleaflet.WKTLayer(\n",
    "        wkt_string=poly[2].wkt,  # Use the correct geometry object\n",
    "        style={\"color\": \"green\"}\n",
    "    )\n",
    "    \n",
    "    m.add_layer(wlayer_3DEP)\n",
    "    wlayer_3DEP_list.append(wlayer_3DEP)\n",
    "    usgs_3dep_datasets.append(poly[1])\n",
    "    \n",
    "    # Estimate total points using ratio of area and point count\n",
    "    number_pts_est.append((int((AOI_StatePlane.area / poly[3].area) * (poly[5]))))\n",
    "\n",
    "# Make ipyleaflet layers from the AOI and add to map\n",
    "wlayer_user = ipyleaflet.WKTLayer(\n",
    "    wkt_string=AOI_GCS.boundary.wkt,\n",
    "    style={\"color\": \"blue\"}\n",
    ")\n",
    "\n",
    "AOI_StatePlane_wkt = AOI_StatePlane.wkt\n",
    "m.add_layer(wlayer_user)\n",
    "\n",
    "# Sum the estimates of the number of points from each 3DEP dataset within the AOI\n",
    "num_pts_est = sum(number_pts_est)\n",
    "\n",
    "# Plot map and specify desired point cloud resolution using a widget\n",
    "user_resolution = widgets.RadioButtons(\n",
    "    options=[\n",
    "        (f'Full - All ~{int(math.ceil(num_pts_est / 1e6) * 1e6):,} points', 1.0),\n",
    "        (f'High - 2m resolution', 2.0),\n",
    "        (f'Mid  - 5m resolution', 5.0),\n",
    "        (f'Low  - 10m resolution', 10.0)\n",
    "    ],\n",
    "    layout={'width': 'max-content'},\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "display(m)\n",
    "print(f'Your AOI at full resolution will include approximately {int(math.ceil(num_pts_est / 1e6) * 1e6):,} points. Select desired point cloud resolution.')\n",
    "widgets.VBox([user_resolution])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not modify AOI_EPSG3857_wkt, usgs_3dep_datasets, or pointcloud_resolution\n",
    "# Modify the optional arguments to fit user need.\n",
    "# Change outCRS to EPSG code of desired coordinate reference system (Default is EPSG:3857 - Web Mercator Projection)\n",
    "# Change pc_outname to descriptive name and pc_outType to 'las' or 'laz'.\n",
    "os.chdir(\"../pc\")\n",
    "pointcloud_resolution = user_resolution.value\n",
    "pc_pipeline = build_pdal_pipeline(AOI_EPSG3857_wkt, usgs_3dep_datasets, pointcloud_resolution, filterNoise = True,\n",
    "                                  reclassify = False, savePointCloud = True, outCRS = 3857, \n",
    "                                  pc_outName = 'pointcloud_test', pc_outType = 'laz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The PDAL pipeline is now constructed. Running the the PDAL Python bindings function ```pdal.Pipeline()``` creates the pdal.Pipeline object from a json-ized version of the pointcloud pipeline we created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_pipeline = pdal.Pipeline(json.dumps(pc_pipeline))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pc_pipeline.execute_streaming(chunk_size=1000000) # use this if reclassify == False \n",
    "#pc_pipeline.execute() # use this if reclassify == True "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the user only desires point cloud data, they may stop here. Following is an overview on how dtm may be created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"Make-Digital-Terrain-Model-(DTM)\"></a>\n",
    "### Make Digital Terrain Model (DTM)\n",
    "The following cells will produce a Digital Terrain Model (DTM), also called a 'bare earth model' using lidar returns classified as 'ground' (USGS Class 2). Do not modify the `AOI_EPSG3857_wkt`, `usgs_3dep_datasets`, or `pointcloud_resolution` arguments. Specify the desired dtm resolution (in meters), the appropriate point cloud processing steps, and the file names/extensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not modify AOI_EPSG3857_wkt, usgs_3dep_datasets, or pointcloud_resolution\n",
    "# Modify the optional arguments to fit user need.\n",
    "# Change outCRS to EPSG code of desired coordinate reference system (Default is EPSG:3857 - Web Mercator Projection)\n",
    "# Change dem_outname to descriptive name and change dem_outExt and driver to desired file type.\n",
    "os.chdir(\"../tif\")\n",
    "pointcloud_resolution = user_resolution.value\n",
    "dtm_resolution = 0.3048  # 1 foot resolution\n",
    "dtm_pipeline = make_DEM_pipeline(AOI_EPSG3857_wkt, usgs_3dep_datasets, pointcloud_resolution, dtm_resolution,\n",
    "                                 filterNoise = True, reclassify = False, savePointCloud = False, outCRS = 3857,\n",
    "                                 pc_outName = 'pointcloud_test', pc_outType = 'laz', demType = 'dtm', \n",
    "                                 gridMethod='idw', dem_outName = 'test_dtm', dem_outExt = 'tif', driver = \"GTiff\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The PDAL pipeline is now constructed for making the DTM. Running the the PDAL Python bindings function ```pdal.Pipeline()``` creates the pdal.Pipeline object from a json-ized version of the pointcloud pipeline we created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm_pipeline = pdal.Pipeline(json.dumps(dtm_pipeline))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dtm_pipeline.execute_streaming(chunk_size=1000000) # use this if reclassify == False \n",
    "#dtm_pipeline.execute() # use this if reclassify == True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Credits\n",
    "This notebook is based on the one made by Cole Speed, Matthew Beckley, Christopher Crosby (UNAVCO, Inc.), and Viswanath Nandigam (San Diego Supercomputer Center). Some of the contents and tools provided here are based on their original work and contributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3dep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
